{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww14580\viewh13900\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 Let\'92s train some networks.\

\f1\b0 \
\
MATLAB Deep Learning Toolbox has a pretty good framework for convnets. \
\
Maybe we start in MATLAB, and then invest an afternoon later on to carry out to the inevitable port to PyTorch so we can use Attention modules, google compute, etc..\
\

\f0\b AlexNet\

\f1\b0 {{\NeXTGraphic Screen Shot 2021-01-21 at 9.14.04 AM.png \width2480 \height2460 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \

\f0\b Plans, from easy to hard:\

\f1\b0 [ ] fold, pairing, sequence (x,p,seq) \'97> energy (E). [a linear model! should work with very simple \'931x1 conv\'94]\
[ ] pairing, seq (p,seq) \'97> free energy (dE) over all conformations with pairing [convnet, or maybe graph-convnet]\
[ ] sequence \'97> bpp. [convnet]\
[ ] Port to numpy/scipy.\
[ ] sequence \'97> bpp. [attention]\
[ ] sequence \'97> bpp. [dynamic programming]\
[ ] sequence \'97> bpp. [dynamic programming + bonuses.]\
[ ] sequence \'97> MFE [transformer]\
[ ] sequence \'97> MFE [transformer with signed features (\'91equivariance\'92)]\
}