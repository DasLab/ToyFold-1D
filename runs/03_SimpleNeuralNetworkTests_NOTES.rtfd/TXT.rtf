{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;\f2\fswiss\fcharset0 Helvetica-Oblique;
}
{\colortbl;\red255\green255\blue255;\red14\green0\blue255;\red170\green4\blue249;\red14\green0\blue255;
\red170\green4\blue249;\red2\green128\blue9;\red2\green128\blue9;}
{\*\expandedcolortbl;;\csgenericrgb\c5490\c0\c100000;\csgenericrgb\c66667\c1569\c97647;\csgenericrgb\c5490\c0\c100000;
\csgenericrgb\c66667\c1569\c97647;\csgenericrgb\c784\c50196\c3529;\csgenericrgb\c784\c50196\c3529;}
\margl1440\margr1440\vieww17400\viewh18200\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 Let\'92s train some networks.\

\f1\b0 \
\
MATLAB Deep Learning Toolbox has a pretty good framework for convnets. \
\
Maybe we start in MATLAB, and then invest an afternoon later on to carry out to the inevitable port to PyTorch so we can use Attention modules, google compute, etc..\
\

\f0\b AlexNet\

\f1\b0 {{\NeXTGraphic Screen Shot 2021-01-21 at 9.14.04 AM.png \width2480 \height2460 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
\

\f0\b Plans, from easy to hard:\

\f1\b0 [x] fold, pairing, sequence (x,p,seq) \'97> energy (E). [a linear model! should work with very simple \'931x1 conv\'94]\
[x] pairing, seq (p,seq) \'97> free energy (dE) over all conformations with pairing [convnet, or maybe graph-convnet]\
[x] sequence \'97> bpp. [convnet]\
[ ] Port to numpy/scipy.\
[ ] sequence \'97> bpp. [attention]\
[ ] sequence \'97> bpp. [dynamic programming]\
[ ] sequence \'97> bpp. [dynamic programming + bonuses.]\
[ ] sequence \'97> MFE [transformer]\
[ ] sequence \'97> MFE [transformer with signed features (\'91equivariance\'92)]\
\
Just predict energy. Should be easy \'97 can use a linear model! But MATLAB training diverges to NaN\
Take x and p features (14+14=28), and fit to E\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     featureInputLayer([28])
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-21 at 5.12.20 PM.png \width11180 \height11500 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\
Drop the ReLU:\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     featureInputLayer([28])
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];\

\fs24 Actually this is OK, though really should go to error of zero.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-21 at 5.14.14 PM.png \width7060 \height11360 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Try 1D CNN. \
Pretend input is 2 layers, \'9114x1 images\'92.\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 1 2])
\fs24 \

\fs20     convolution2dLayer([3 1],2)
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];\

\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-21 at 5.15.59 PM.png \width7920 \height6260 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 \
Maybe too many params in fullyConnected \'97 try to globalAverage before fully connected \'85 that should reflect the idea that we\'92re just summing bend energy (based on x), summing  pairs (based on p). \
\
layers = [ 
\fs24 \

\fs20     imageInputLayer([14 1 2])
\fs24 \

\fs20     convolution2dLayer([3 1],2)
\fs24 \

\fs20     globalAveragePooling2dLayer
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \

\fs20 \
Uh not much of a boost:\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 {{\NeXTGraphic Screen Shot 2021-01-21 at 5.19.03 PM.png \width8400 \height6000 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Try \'91deep\'92 net?\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 1 2])
\fs24 \

\fs20     convolution2dLayer([3 1],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 1],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     globalAveragePooling2dLayer
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-21 at 5.22.55 PM.png \width13660 \height10060 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Try \'91deep\'92 net with 10 features rather than 2 in each layer.\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 1 2])
\fs24 \

\fs20     convolution2dLayer([3 1],10)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 1],10)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     globalAveragePooling2dLayer
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \

\fs20 \
Yea this is on the right track.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 {{\NeXTGraphic Screen Shot 2021-01-21 at 5.25.40 PM.png \width12880 \height9120 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-21 at 5.27.55 PM.png \width3960 \height3760 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
How about 3 layers?\
\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 1 2])
\fs24 \

\fs20     convolution2dLayer([3 1],10)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 1],10)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 1],10)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     globalAveragePooling2dLayer
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-21 at 5.30.25 PM.png \width11700 \height6900 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
In above, encoding of p is a bit wonky \'97 its the partner.\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b \cf0 \ul \ulc0 How about a legit 2D convnet?\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1\b0 \cf0 \ulnone Reshape input to 3 layers: \
 \'95 1 layer with x tiled row-wise\
 \'95 1 layer with x tiled column-wise \
 \'95 P = 1,0 matrix.\
\
train_idx = [1:500];\
test_idx = [9001:10000];\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 3])
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
Note \'97 I tried some architectures with fewer layers or no non-linearities and didn\'92t see success.\
Looks OK \'97 try more features in each layer, deeper net?\
Note also train/test discrepancy! Looks like overfitting May need more data.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-21 at 5.34.42 PM.png \width11120 \height9020 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 3])
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 Amazingly, just 2 features in each layer.\
Repeat above with 5000 training examples instead of 500. Looks great!\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-21 at 6.11.22 PM.png \width13960 \height6900 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-21 at 6.59.38 PM.png \width3840 \height3680 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Gets even better if we add a layer.\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 2])
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b \cf0 \
Nice!
\f1\b0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-27 at 12.46.54 PM.png \width13440 \height8140 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ul 27 Jan, 2021\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ulnone Can the convnet do more?\
Can we infer energy with just conformation? I.e., no pairing info.\
Uh no\'85 maybe it needs sequence to figure out pairing.  Following with two hidden conv2d layers\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-27 at 10.06.55 AM.png \width14080 \height9020 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
Doesn\'92t get better with 3 layers.\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-27 at 12.53.17 PM.png \width13320 \height7800 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
How about the opposite \'97 if we know pairing, is that enough to get energy?\
YEs, sort of. Clearly overfitting know \'97 look at training vs. test error. \
Is that because MFE conformation actually is degenerate in energy given pairing? 
\f0\b Doesn\'92t totally make sense to me.\
\pard\pardeftab720\partightenfactor0

\f1\b0\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 3])
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-27 at 10.13.18 AM.png \width11980 \height9440 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
More layers? More features? 
\f2\i Yes, actually that does it! 
\f1\i0 \
The extra layers build up mer complex features that appear able to infer the structure information (needed to estimate bending component of energy):
\f2\i \
\pard\pardeftab720\partightenfactor0

\f1\i0\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 1])
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-27 at 10.23.51 AM.png \width12700 \height5860 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-27 at 10.24.48 AM.png \width3620 \height3560 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
[Later, Feb. 7 2021, was able to reproduce above, without issue\
{{\NeXTGraphic Screen Shot 2021-02-07 at 1.55.53 PM.png \width12360 \height8520 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
]\
\
\
\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ul Revisit predicting Energy from conformation (x) and 
\f2\i sequence. 
\f1\i0 I.e. ask for pairings\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b \cf0 \ulnone Still doesn\'92t work? 
\f1\b0 But it should be trivial to figure out pairings.
\f0\b \
\pard\pardeftab720\partightenfactor0

\f1\b0\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 10])
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-27 at 12.53.17 PM.png \width13320 \height7800 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 10])
\fs24 \

\fs20     convolution2dLayer([3 3],10)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],10)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],10)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-27 at 4.37.59 PM.png \width13240 \height4960 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Oh come on, this has to work. Neural net just has to learn that A/U and G/C at the same level are likely to pair. How hard is that? Should be easy to encode into a convnet.\
Yes \'97 just had to increase # early layers:\
The idea is that we need more layers to ensure we remember the X-data while learning (in early layers) to infer pairing.\
\pard\pardeftab720\partightenfactor0

\fs20 \cf2 end
\fs24 \cf0 \
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 10])
\fs24 \

\fs20     convolution2dLayer([3 3],10)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],10)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-27 at 4.53.58 PM.png \width11480 \height7320 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Tried to rerun again, and it didn\'92t train!?\
And, argh I forgot to save the net above.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-27 at 7.12.22 PM.png \width14380 \height9140 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
Should be able to directly train a neural net to fit P based on sequence and X:\
\
Check that X levels match (channels 1 and 2 for x at 1 and x at 2)  and\
 that sequences are allowed to form base pairs (channels 3-10 for A at 1-U at 2, etc.):\
\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 imagesc(D2(:,:,1,10)==D2(:,:,2,10) & \cf2 ...
\fs24 \cf0 \

\fs20     ( (D2(:,:,3,10)==1 & D2(:,:,10,10)==1) | \cf2 ...
\fs24 \cf0 \

\fs20       (D2(:,:,5,10)==1 & D2(:,:,8,10)==1) | \cf2 ...
\fs24 \cf0 \

\fs20       (D2(:,:,7,10)==1 & D2(:,:,6,10)==1) | \cf2 ...
\fs24 \cf0 \

\fs20       (D2(:,:,9,10)==1 & D2(:,:,4,10)==1) ) );\
\
This is basically a bunch of \'931x1\'94 convs
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-27 at 7.34.08 PM.png \width15060 \height6640 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
However, not able to train\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 10])
\fs24 \

\fs20     convolution2dLayer([1 1],10,\cf3 'Padding'\cf0 ,\cf3 'Same'\cf0 ,\cf3 'Name'\cf0 ,\cf3 'convInp'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([1 1],10,\cf3 'Padding'\cf0 ,\cf3 'Same'\cf0 ,\cf3 'Name'\cf0 ,\cf3 'conv2'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([1 1],10,\cf3 'Padding'\cf0 ,\cf3 'Same'\cf0 ,\cf3 'Name'\cf0 ,\cf3 'conv2'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([1 1],10,\cf3 'Padding'\cf0 ,\cf3 'Same'\cf0 ,\cf3 'Name'\cf0 ,\cf3 'conv2'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([1 1],10,\cf3 'Padding'\cf0 ,\cf3 'Same'\cf0 ,\cf3 'Name'\cf0 ,\cf3 'conv2'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-01-27 at 7.48.14 PM.png \width15940 \height9840 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ul Feb. 7, 2021\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ulnone Let\'92s do some ridiculously easy sanity checks.\
Can we learn to identify X(1) = X(2)? Straightup equality? No?\
\
\pard\pardeftab720\partightenfactor0

\fs20 \cf4 for\cf0  n = 1:N
\fs24 \
\pard\pardeftab720\partightenfactor0

\fs20 \cf0     D2(:,:,1,n) = repmat(all_x(:,n),[1 14]);
\fs24 \

\fs20     D2(:,:,2,n) = repmat(all_x(:,n)',[14 1]);
\fs24 \
\pard\pardeftab720\partightenfactor0

\fs20 \cf4 end
\fs24 \cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 Xequal = (D2(:,:,1,:)==D2(:,:,2,:));
\fs24 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 2])
\fs24 \

\fs20     convolution2dLayer([1 1],1)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.05.29 PM.png \width13660 \height7360 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
if I jitter by randn, \
\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 D2 = D2 + randn(14,14,2,10000);
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
it still does not train\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.08.34 PM.png \width14140 \height7220 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\pard\pardeftab720\partightenfactor0
\cf0 \
Uh that\'92s crazy. But apparently there\'92s precedent \'97 2018 arXv paper: \
\
https://arxiv.org/pdf/1812.01662.pdf \
Basic binary relations such as equality and inequality are fundamental to relational\
data structures. Neural networks should learn such relations and generalise to\
new unseen data. We show in this study, however, that this generalisation fails\
with standard feed-forward networks on binary vectors. Even when trained with\
maximal training data, standard networks do not reliably detect equality\'85.\
\
Try convolution, just in case that helps with more info. Nope:\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 2])
\fs24 \

\fs20     convolution2dLayer([3 3],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.12.16 PM.png \width15200 \height5180 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
Deeper net?\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 2])
\fs24 \

\fs20     convolution2dLayer([1 1],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([1 1],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 Nope:\
{{\NeXTGraphic Screen Shot 2021-02-07 at 12.13.12 PM.png \width12300 \height6760 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
Back to:\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 3])
\fs24 \

\fs20     convolution2dLayer([1 1],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\
What if we give it the answer?\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 D2(:,:,3,:) =  D2(:,:,1,:)-D2(:,:,2,:);
\fs24 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
{{\NeXTGraphic Screen Shot 2021-02-07 at 12.15.10 PM.png \width13560 \height8300 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 \
Try the \'91Differential ReLU\'92 in above paper as an input feature, abs(x1-x2):\
D2(:,:,3,:) =  abs(D2(:,:,1,:)-D2(:,:,2,:)); \cf6 % Just give it the answer!\
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.17.03 PM.png \width16300 \height5620 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Uh, sanity check \'97 provide the actual answer:\
\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 D2(:,:,3,:) =  Xequal; \cf6 % Just give it the answer!
\fs24 \cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 OK, yes this \'91trains\'92. \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.19.06 PM.png \width13640 \height6420 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
This is puzzling. Let\'92s go back to including abs(x1-x2) \
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 D2(:,:,3,:) =  abs(D2(:,:,1,:)-D2(:,:,2,:)); \cf7 % Just give it the answer!\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 And give more data (5000 training examples)\'85 taking forever, and doesn\'92t look too promising\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.21.54 PM.png \width13440 \height7820 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Another sanity check \'97 on jitter. Provide the answer as input feature, but jitter:\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 D2(:,:,3,:) =  Xequal + randn(14,14,1,10000); \cf6 % Just give it the answer with jitter
\fs24 \cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.24.48 PM.png \width17640 \height6180 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
Hmm doesn\'92t train!\
\
What if Ilower jitter on input:\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 D2(:,:,3,:) =  Xequal + 0.1*randn(14,14,1,10000); \cf6 % Just give it the answer with jitter\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 Yea better
\fs20 \cf6 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.27.17 PM.png \width20440 \height9880 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Hmm may need to lower jitter on input:\
\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 Xequal = double(D2(:,:,1,:)==D2(:,:,2,:));
\fs24 \

\fs20  
\fs24 \

\fs20 D2 = D2 + 0.1*randn(14,14,2,10000);
\fs24 \

\fs20 D2(:,:,3,:) =  abs(D2(:,:,1,:)-D2(:,:,2,:)); \cf6 % Just give it the answer!
\fs24 \cf0 \

\fs20 D2(:,:,3,:) =  Xequal + 0.1*randn(14,14,1,10000); \cf6 % Just give it the answer with jitter
\fs24 \cf0 \

\fs20  
\fs24 \

\fs20 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 3])
\fs24 \

\fs20     convolution2dLayer([1 1],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([1 1],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.30.07 PM.png \width12500 \height5640 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
OK how about now using abs(x1-x2) input, jitter by 0.1:\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0  
\fs24 \

\fs20 D2 = D2 + 0.1*randn(14,14,2,10000);
\fs24 \

\fs20 D2(:,:,3,:) =  abs(D2(:,:,1,:)-D2(:,:,2,:)); \cf6 % Just give it the answer!
\fs24 \cf0 \
\pard\pardeftab720\partightenfactor0

\fs20 \cf6 %D2(:,:,3,:) =  Xequal + 0.1*randn(14,14,1,10000); % Just give it the answer with jitter
\fs24 \cf0 \
\pard\pardeftab720\partightenfactor0

\fs20 \cf0  
\fs24 \

\fs20 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 3])
\fs24 \

\fs20     convolution2dLayer([1 1],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([1 1],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.33.01 PM.png \width14360 \height6740 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Wait, did this really train? Yea basically:\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.36.09 PM.png \width6720 \height3360 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Interestingly, the second layer was key.. one layer doesn\'92t train well:\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 3])
\fs24 \

\fs20     convolution2dLayer([1 1],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.38.53 PM.png \width14340 \height6800 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.39.08 PM.png \width6640 \height3340 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
Does larger field of view help? (Might integrate away noise)\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 3])
\fs24 \

\fs20     convolution2dLayer([3 3],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 Yes! Though note that answer depends on starting conditions\'85 Following are two runs of the exact same training:\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.42.17 PM.png \width14120 \height8100 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.43.15 PM.png \width14600 \height7560 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.44.29 PM.png \width6700 \height3420 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Actually, does jitter help or hurt?\
A few runs with the same net as above, without jittering input \'97does achieve low RMSD in one of 3 tries:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.52.10 PM.png \width14240 \height6540 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.52.57 PM.png \width14440 \height5420 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 12.53.59 PM.png \width13280 \height6360 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\

\f0\b OK, let\'92s now revisit training on P  (pairing matrix) based on X and sequence.\

\f1\b0 Add in feature for abs(X1-X2):\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 D2(:,:,11,:) =  abs(D2(:,:,1,:)-D2(:,:,2,:)); 
\fs24 \

\fs20  
\fs24 \

\fs20 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 11])
\fs24 \

\fs20     convolution2dLayer([3 3],10,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([1 1],10,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([1 1],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 train_idx = [1:5000];
\fs24 \

\fs20 test_idx = [9001:10000];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 1.05.27 PM.png \width9820 \height7720 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 1.04.51 PM.png \width6820 \height3380 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
Why not perfect? Let\'92s plot error across 10000 data\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 1.06.50 PM.png \width6820 \height3220 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
Let\'92s take a look at the problem cases:\
One from training set \'97 misses a singlet base pair\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 1.07.46 PM.png \width6840 \height3400 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Worst case overall:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 1.08.16 PM.png \width6700 \height3420 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 Haha, the last one is worst case \'97 isn\'92t even symmetric!\
Idx = 9447.\
But that looks so bizarre. What does X look like ?\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 1.09.49 PM.png \width6760 \height3340 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \

\f0\b GAUCCGCCCUAGGU
\f1\b0 \
Must be forming parallel strands! \
Maybe should disallow that in ToyFold1D!\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 Let\'92s run again, jitter input by 0.1 * randn. And let training complete.\
Obviously overfits\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 1.26.13 PM.png \width15440 \height9300 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Still not awful on test set. Here\'92s that worst case scenario again..\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 1.25.32 PM.png \width6900 \height3380 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 1.25.50 PM.png \width6580 \height3420 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\ul Revisit predicting energy from conformation and sequence\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \ulnone Check again that we get fail with pretty elaborate convent:\
\
layers = [ \
    imageInputLayer([14 14 10])\
    convolution2dLayer([3 3],10)\
    reluLayer\
    convolution2dLayer([3 3],10)\
    reluLayer\
    convolution2dLayer([3 3],2)\
    reluLayer\
    convolution2dLayer([3 3],2)\
    reluLayer\
    convolution2dLayer([3 3],2)\
    reluLayer\
    fullyConnectedLayer(1)\
    regressionLayer\
    ];\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 1.43.06 PM.png \width16100 \height10780 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 Yup, fail, and jitter doesn\'92t help.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 1.44.46 PM.png \width13540 \height9620 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
Now with\
abs( x1-x2)\
 supplementing input feature. \
\
Darn! Doesn\'92t train.\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 1.47.01 PM.png \width13260 \height7880 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Also tried to do a run without x1, x2 features \'97 just abs(x1-x2) and sequence tiles in x and y:\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 9])
\fs24 \

\fs20     convolution2dLayer([3 3],10)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],10)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],2)
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     fullyConnectedLayer(1)
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 2.12.31 PM.png \width12000 \height8060 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Darn, I give up.\
\
\

\f0\b \ul Try to fit BPP
\f1\b0 \ulnone \
Go from sequence to BPP \'97 a truly nontrivial inference problem\'85\
\
No beans with a naive implementation:\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 8])
\fs24 \

\fs20     convolution2dLayer([3 3],10,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],10,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],10,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],10,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],10,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 2.25.39 PM.png \width15320 \height10220 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 8])
\fs24 \

\fs20     convolution2dLayer([3 3],64,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],64,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],64,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 2.46.45 PM.png \width16240 \height15100 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\

\f0\b Predefine what can pair?
\f1\b0  Set that as an additional input layer.\
That seems to be working better:\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 9])
\fs24 \

\fs20     convolution2dLayer([3 3],10,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],10,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],10,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],10,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],10,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
Except for a weird spike at end. Lets rerun\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 3.37.59 PM.png \width18000 \height9380 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
OK nice\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 3.52.47 PM.png \width15180 \height7860 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
Take a look at some predictions \'97 not too bad! Last one below is worst case.\
{{\NeXTGraphic Screen Shot 2021-02-07 at 3.48.50 PM.png \width6680 \height3420 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
{{\NeXTGraphic Screen Shot 2021-02-07 at 3.49.06 PM.png \width6760 \height3520 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
{{\NeXTGraphic Screen Shot 2021-02-07 at 3.49.16 PM.png \width6500 \height3400 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
{{\NeXTGraphic Screen Shot 2021-02-07 at 3.52.06 PM.png \width6580 \height3340 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\
\
\
\
\
What if we apply sigmoid at end? No, looks like vanishing gradients:\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 3.57.18 PM.png \width12260 \height7240 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 Try to train longer\'85 weird \'97 most starts do not lead to training!\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 4.12.31 PM.png \width16620 \height7900 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 Tried to start from checkpoint, but crazy fluctuations occurred immediately. Try one more time.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 6.06.59 PM.png \width17780 \height10020 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
SAVED! As BPPnet.\
\
Can we train with more channels per layer? Keep 6 conv layers.\
\pard\pardeftab720\partightenfactor0
\cf0 Didn\'92t see much luck until I added 
\f0\b batch norm layers
\f1\b0\fs28 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 \
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 9])
\fs24 \

\fs20     convolution2dLayer([3 3],16,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],16,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],16,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],16,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],16,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 6.53.41 PM.png \width14080 \height5660 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \'92\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 6.54.36 PM.png \width6880 \height3400 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
Worse error \'97 why? Maybe because connection is png range \'97 2 to 12.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 6.55.21 PM.png \width6560 \height3460 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
Actually let\'92s go back to 8 channels per layer \'97 is training more robust with batch norm? Yes, though final RMSE is not that great.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 7.17.10 PM.png \width14440 \height7040 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
So now can we go deeper? \'97 and capture full field of view (base pairs extending all the way across)? \
Try 12 layers.\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 9])
\fs24 \

\fs20     convolution2dLayer([3 3],8,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],8,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],8,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],8,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],8,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],8,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],8,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],8,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],8,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],8,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],8,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],8,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];
\fs24 \
Not much better than above\'85\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 7.34.24 PM.png \width13720 \height7060 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Interesting question \'97 since batch norm helps reduce dependence on initial start conditions, do we really need to still \'91hand-craft\'92 the \'91can_pair\'92 feature? Yes, actually!\
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 layers = [ 
\fs24 \

\fs20     imageInputLayer([14 14 8])
\fs24 \

\fs20     convolution2dLayer([3 3],16,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],16,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],16,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],16,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],16,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     convolution2dLayer([3 3],1,\cf5 'Padding'\cf0 ,\cf5 'Same'\cf0 )
\fs24 \

\fs20     batchNormalizationLayer
\fs24 \

\fs20     reluLayer
\fs24 \

\fs20     regressionLayer
\fs24 \

\fs20     ];\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 7.50.25 PM.png \width11820 \height8180 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Could also try ResNet, etc.\
Though probably time to stress test above with some larger sequences\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
And\'85 code up  in pytorch and move into attention.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
Easy to test on shorter sequences:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 7.57.36 PM.png \width4260 \height14240 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
For longer sequences, e.g., 25mers, needed to train net on 25x25 inputs. Can\'92t expand the net after the fact!?\
Oops.\
25mers take forever, but could get a few 17mers \'85 started taking some notes in 04_LongRNAValidationTests\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2021-02-07 at 8.08.44 PM.png \width7360 \height8360 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
}